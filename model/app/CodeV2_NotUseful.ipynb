{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97c58148-6d63-4047-9313-cad80ffd4e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import shutil\n",
    "import torch\n",
    "import torchvision\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim  \n",
    "#import datasets\n",
    "#from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fd7d843-043d-4e13-99b5-77bacbca1548",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m                 save_spectrogram(file, img_path)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Process the files and split them\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m files_by_class \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m split_and_save(files_by_class)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing complete. Spectrogram images are saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 28\u001b[0m, in \u001b[0;36mprocess_files\u001b[0;34m(source_dir)\u001b[0m\n\u001b[1;32m     25\u001b[0m files_by_class \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m: []}\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Iterate through all .wav files in source directory\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     30\u001b[0m         label \u001b[38;5;241m=\u001b[39m filename[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# B, I, or F from the filename\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset'"
     ]
    }
   ],
   "source": [
    "source_dir = 'dataset'  # Directory containing your .wav files\n",
    "output_dir = 'pleasework'  # Directory to save spectrogram images\n",
    "train_dir = os.path.join(output_dir, 'train')\n",
    "valid_dir = os.path.join(output_dir, 'valid')\n",
    "test_dir = os.path.join(output_dir, 'test')\n",
    "\n",
    "# Create necessary directories\n",
    "for folder in [train_dir, valid_dir, test_dir]:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "def save_spectrogram(wav_path, img_path):\n",
    "    # Load audio file\n",
    "    y, sr = librosa.load(wav_path, sr=None)\n",
    "    # Generate spectrogram\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "    # Plot spectrogram\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(D, x_axis='time', y_axis='log', sr=sr)\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    # Save spectrogram as an image\n",
    "    plt.savefig(img_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "def process_files(source_dir):\n",
    "    files_by_class = {'B': [], 'I': [], 'F': []}\n",
    "\n",
    "    # Iterate through all .wav files in source directory\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.endswith('.wav'):\n",
    "            label = filename[0]  # B, I, or F from the filename\n",
    "            if label in files_by_class:\n",
    "                files_by_class[label].append(os.path.join(source_dir, filename))\n",
    "    \n",
    "    return files_by_class\n",
    "\n",
    "def split_and_save(files_by_class):\n",
    "    for label, files in files_by_class.items():\n",
    "        # Split the files into train, test, and valid sets (80%, 10%, 10%)\n",
    "        train_files, temp_files = train_test_split(files, test_size=0.3, random_state=42)\n",
    "        valid_files, test_files = train_test_split(temp_files, test_size=0.4, random_state=42)\n",
    "\n",
    "        # Save files to respective directories\n",
    "        for folder, file_list in zip([train_dir, valid_dir, test_dir], [train_files, valid_files, test_files]):\n",
    "            for file in file_list:\n",
    "                # Create spectrogram and save it\n",
    "                img_filename = os.path.splitext(os.path.basename(file))[0] + '.png'\n",
    "                img_path = os.path.join(folder, label, img_filename)\n",
    "                os.makedirs(os.path.dirname(img_path), exist_ok=True)\n",
    "                save_spectrogram(file, img_path)\n",
    "\n",
    "# Process the files and split them\n",
    "files_by_class = process_files(source_dir)\n",
    "split_and_save(files_by_class)\n",
    "\n",
    "print(\"Preprocessing complete. Spectrogram images are saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b57e221-fa97-4809-8b86-1c6549a7bb03",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m ImageFolder(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpleasework/train\u001b[39m\u001b[38;5;124m\"\u001b[39m, transform\u001b[38;5;241m=\u001b[39m\u001b[43mtransform\u001b[49m)\n\u001b[0;32m      2\u001b[0m valid_dataset \u001b[38;5;241m=\u001b[39m ImageFolder(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpleasework/valid\u001b[39m\u001b[38;5;124m\"\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Merge datasets\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transform' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = ImageFolder(root=\"pleasework/train\", transform=transform)\n",
    "valid_dataset = ImageFolder(root=\"pleasework/valid\", transform=transform)\n",
    "\n",
    "# Merge datasets\n",
    "combined_dataset = torch.utils.data.ConcatDataset([train_dataset, valid_dataset])\n",
    "\n",
    "\n",
    "# Set parameters\n",
    "data_dir = \"pleasework/train\"\n",
    "k_folds = 4\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fold_results = []\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(combined_dataset)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{k_folds} ---\")\n",
    "    \n",
    "    # Create data subsets\n",
    "    train_subset = Subset(combined_dataset, train_idx)\n",
    "    valid_subset = Subset(combined_dataset, valid_idx)\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_subset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Load MobileNetV2 model\n",
    "    model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, 3)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    fold_accuracy = 100 * correct / total\n",
    "    fold_results.append(fold_accuracy)\n",
    "    print(f\"Validation Accuracy for Fold {fold+1}: {fold_accuracy:.2f}%\")\n",
    "\n",
    "# Print average accuracy across folds\n",
    "print(f\"\\nAverage K-Fold Accuracy: {np.mean(fold_results):.2f}%\")\n",
    "\n",
    "# Load test set\n",
    "test_dataset = ImageFolder(root=\"pleasework/test\", transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\n--- Final Test Evaluation ---\")\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=test_dataset.classes, yticklabels=test_dataset.classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - Final Test Set')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec160f28-4fec-44e6-9f61-6302213cde64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
